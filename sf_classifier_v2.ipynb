{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np, pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cor.json') as data:\n",
    "    file = json.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dialogues = [] \n",
    "for d in file[:2]:\n",
    "    samples = defaultdict(dict)\n",
    "    result = d['completions'][0]['result']\n",
    "    texts_without_labels = d['data']['text']\n",
    "    for sample in result:\n",
    "        speaker = texts_without_labels[int(sample['value']['start'])]['speaker']\n",
    "        samples[sample['id']]['speaker'] = speaker\n",
    "        samples[sample['id']]['text'] = sample['value']['text']\n",
    "        samples[sample['id']]['start'] = int(sample['value']['start'])\n",
    "        if 'paragraphlabels' in sample['value']:\n",
    "            samples[sample['id']]['paragraphlabels'] = sample['value']['paragraphlabels'][0]\n",
    "        if 'choices' in sample['value']:\n",
    "            samples[sample['id']]['choices'] = sample['value']['choices'][0]\n",
    "    \n",
    "    sorted_samples = sorted([(samples[sample_id]['start'], sample_id) for sample_id in samples])\n",
    "    texts = []\n",
    "    labels = []\n",
    "    speakers = []\n",
    "    for _, sample_id in sorted_samples:\n",
    "        if samples[sample_id]['text'] != 'PAUSE':\n",
    "            texts.append(str(samples[sample_id]['text']).replace('\\n', ''))\n",
    "            speakers.append(samples[sample_id]['speaker'])\n",
    "            paragraph_labels = samples[sample_id].get('paragraphlabels', '')\n",
    "            choices = samples[sample_id].get('choices', '')\n",
    "            labels.append(paragraph_labels + '.' + choices)\n",
    "    dialogues.append((texts, labels, speakers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dialogues[1][0]\n",
    "test_data = dialogues[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = dialogues[1][1]\n",
    "test_labels = dialogues[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_odds(list_with_lines):\n",
    "    for i in range(len(list_with_lines)):        \n",
    "        if 'them' in list_with_lines[i]:\n",
    "            list_with_lines[i] = list_with_lines[i].replace('them', ' them ')\n",
    "        if ' em ' in list_with_lines[i]:\n",
    "            list_with_lines[i] = list_with_lines[i].replace(' em ', ' them ')\n",
    "        if 'laugh' in list_with_lines[i]:\n",
    "            list_with_lines[i] = list_with_lines[i].replace('laugh', '')\n",
    "        if 'uh?' in list_with_lines[i]:\n",
    "            list_with_lines[i] = list_with_lines[i].replace('uh?', '')\n",
    "        if 'ʔuh' in list_with_lines[i]:\n",
    "            list_with_lines[i] = list_with_lines[i].replace('ʔuh', '')\n",
    "        if 'ʔ' in list_with_lines[i]:\n",
    "            list_with_lines[i] = list_with_lines[i].replace('ʔ', '')\n",
    "    return list_with_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_lines_sust=[]\n",
    "sustains=[]\n",
    "sus_tags=[]\n",
    "for i in range(len(train_data)):\n",
    "    if 'Sustain' in train_labels[i]:\n",
    "            previous_lines_sust.append(train_data[i-1])\n",
    "            sustains.append(train_data[i])\n",
    "            sus_tags.append(train_labels[i])\n",
    "for i in range(len(test_data)):\n",
    "    if 'Sustain' in test_labels[i]:\n",
    "        previous_lines_sust.append(test_data[i-1])\n",
    "        sustains.append(test_data[i])\n",
    "        sus_tags.append(test_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sus_tags)):\n",
    "    if 'Append' in sus_tags[i]:\n",
    "        sus_tags[i]=re.sub('Append','Prolong', sus_tags[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "sustains=delete_odds(sustains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "responds=[]\n",
    "previous_responds=[]\n",
    "respond_tags=[]\n",
    "for i in range(len(train_data)):\n",
    "    if 'Answer' in train_labels[i]:\n",
    "        continue\n",
    "    if 'Disengage' in train_labels[i]:\n",
    "        continue\n",
    "    elif 'Respond' in train_labels[i]:\n",
    "        responds.append(train_data[i])\n",
    "        previous_responds.append(train_data[i-1])\n",
    "        respond_tags.append(train_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_data)):\n",
    "    if 'Answer' in test_labels[i]:\n",
    "        continue\n",
    "    if 'Disengage' in test_labels[i]:\n",
    "        continue\n",
    "    elif 'Respond' in test_labels[i]:\n",
    "        responds.append(test_data[i])\n",
    "        previous_responds.append(test_data[i-1])\n",
    "        respond_tags.append(test_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_responds(respond_tags):\n",
    "    for i in range(len(respond_tags)):\n",
    "        if 'Decline' in respond_tags[i]:\n",
    "            respond_tags[i]=re.sub('Decline','Contradict',respond_tags[i])\n",
    "        if 'Accept' in respond_tags[i]:\n",
    "            respond_tags[i]=re.sub('Accept','Affirm',respond_tags[i])\n",
    "        tag_list=respond_tags[i].split('.')[-2:]\n",
    "        respond_tags[i]='.'.join(tag_list)\n",
    "    return respond_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "respond_tags=clean_responds(respond_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_lines=[]\n",
    "replies=[]\n",
    "tags=[]\n",
    "for i in range(len(train_data)):\n",
    "    if 'Reply' in train_labels[i]:\n",
    "        if '?' not in train_labels[i]:\n",
    "            previous_lines.append(train_data[i-1])\n",
    "            replies.append(train_data[i])\n",
    "            tags.append(train_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_data)):\n",
    "    if 'Reply' in test_labels[i]:\n",
    "        if '?' not in test_labels[i]:\n",
    "            previous_lines.append(test_data[i-1])\n",
    "            replies.append(test_data[i])\n",
    "            tags.append(test_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tags)):\n",
    "    tag_list = tags[i].split('.')[-2:]\n",
    "    tags[i]=str('.'.join(tag_list))\n",
    "    if 'Answer' in tags[i]:\n",
    "        tags[i]='Response.Resolve.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_speakers=dialogues[1][2]\n",
    "test_speakers = dialogues[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = delete_odds(train_data)\n",
    "test_data = delete_odds(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cut_labels(labels):\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i].startswith('Open'):\n",
    "            labels[i] = 'Open.'\n",
    "        if labels[i].startswith('React.Rejoinder.'):\n",
    "            labels[i] = 'React.Rejoinder.'\n",
    "        if labels[i].startswith('React.Respond.'):\n",
    "            labels[i] = 'React.Respond.'\n",
    "        if labels[i].startswith('Sustain.Continue.'):\n",
    "            labels[i] = 'Sustain.Continue.'\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_train_labels = get_cut_labels(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_test_labels = get_cut_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/bert-base-cased-conversational\")\n",
    "embed_model = AutoModel.from_pretrained(\"DeepPavlov/bert-base-cased-conversational\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(data):\n",
    "    outputs = []\n",
    "    for text in data:\n",
    "        with torch.no_grad():\n",
    "            input_ph= tokenizer(text, padding=True, truncation=True, max_length=30,return_tensors=\"pt\")\n",
    "            output_ph = embed_model(**input_ph)\n",
    "    #        train_outputs.append(output_ph.pooler_output.cpu().numpy()) \n",
    "            sentence_embedding = output_ph.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        outputs.append(sentence_embedding)\n",
    "    outputs = np.concatenate(outputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs=[]\n",
    "all_outputs.extend(get_embeddings(train_data))\n",
    "all_outputs.extend(get_embeddings(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cuts=[]\n",
    "all_cuts.extend(cut_train_labels)\n",
    "all_cuts.extend(cut_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = '/res/SBC058.json'\n",
    "with open(path2) as proba:\n",
    "    proba=json.load(proba)\n",
    "    dialogue = proba['text']\n",
    "    dialogue = [i for i in dialogue if not (i['phrase'] == 'PAUSE')]\n",
    "    phrases = []\n",
    "    speakers=[]\n",
    "    for i in range(len(dialogue)):\n",
    "        phrases.append(dialogue[i]['phrase'])\n",
    "        speakers.append(dialogue[i]['speaker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = delete_odds(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = get_embeddings(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "model = LogisticRegression(C=0.01,class_weight='balanced')\n",
    "model.fit(all_outputs,all_cuts)\n",
    "y_pred_proba = model.predict_proba(test_embeddings)\n",
    "y_pred = model.predict(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =list(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Develop\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]=='Sustain.Continue.':\n",
    "        if speakers[i]!=speakers[i-1]:\n",
    "            if y_pred[i-1]=='Sustain.Continue.':\n",
    "                y_pred[i]='React.Respond.Develop.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open is always the first one\n",
    "y_pred[0]='Open.'\n",
    "first_speaker=speakers[0]\n",
    "for i in range(len(speakers)):\n",
    "    if speakers[i]==first_speaker:\n",
    "        y_pred[i]='Open.'\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "interrogative_words = ['whose', 'what', 'which', 'who', 'whom', 'what', 'which','why', 'where', 'when', 'how']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect all questions in phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions=[]\n",
    "for i in range(len(phrases)):\n",
    "    if '?' in phrases[i]:\n",
    "        questions.append(phrases[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('track_list') as track_list:\n",
    "    track_list= track_list.readlines()\n",
    "train_que =[]\n",
    "train_tags=[]\n",
    "for line in track_list:\n",
    "    line = line.split('/')\n",
    "    train_que.append(line[0])\n",
    "    train_tags.append(line[1][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_em_que = get_embeddings(train_que)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embeddings = get_embeddings(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "lr2 = LogisticRegression(C=0.01, class_weight='balanced')\n",
    "lr2.fit(train_em_que,train_tags)\n",
    "tags_for_track = lr2.predict(question_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_tracks = {'1':'Track.Check', '2':'Track.Confirm', '3':'Track.Clarify','4':'Track.Probe'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tracks(tags_for_track, true_tags):\n",
    "    real_tracks =[]\n",
    "    for i in range(len(list(tags_for_track))):\n",
    "        if tags_for_track[i]=='5':\n",
    "            real_tracks.append(tags_for_track[i])\n",
    "        else:\n",
    "            real_tracks.append(true_tracks[tags_for_track[i]])\n",
    "    return real_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_for_track=map_tracks(tags_for_track,true_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(phrases)):\n",
    "    if phrases[i] in questions:\n",
    "        que_index = questions.index(phrases[i])\n",
    "        if speakers[i]!=speakers[i-1]:\n",
    "            if y_pred[i]=='React.Respond.' and tags_for_track[que_index]!='5':\n",
    "                y_pred[i]='React.Rejoinder.'+tags_for_track[que_index]\n",
    "            if  y_pred[i]=='React.Rejoinder.' and tags_for_track[que_index]!='5':\n",
    "                y_pred[i]+=tags_for_track[que_index]\n",
    "            if y_pred[i]=='React.Rejoinder.' and tags_for_track[que_index]=='5':\n",
    "                for word in interrogative_words:\n",
    "                        if word in phrases[i]:\n",
    "                            y_pred[i]='React.Rejoinder.Rebound'\n",
    "                        else:\n",
    "                            y_pred[i]='React.Rejoinder.Re-challenge'\n",
    "            if y_pred[i]=='Sustain.Continue.':\n",
    "                y_pred[i]='React.Rejoinder.'+tags_for_track[que_index]\n",
    "            if y_pred[i]=='Open.':\n",
    "                pass\n",
    "        if speakers[i]==speakers[i-1]:\n",
    "            if  y_pred[i]=='React.Rejoinder.' and tags_for_track[que_index]!='5':\n",
    "                y_pred[i]+=tags_for_track[que_index]\n",
    "            if y_pred[i]=='React.Rejoinder.' and tags_for_track[que_index]=='5':\n",
    "                for word in interrogative_words:\n",
    "                        if word in phrases[i]:\n",
    "                            y_pred[i]='React.Rejoinder.Rebound'\n",
    "                        else:\n",
    "                            y_pred[i]='React.Rejoinder.Re-challenge'\n",
    "            if y_pred[i]=='Open.':\n",
    "                pass\n",
    "            if y_pred[i]=='Sustain.Continue.':\n",
    "                y_pred[i]='Sustain.Continue.Monitor'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGISTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(speakers)):\n",
    "    if '?' not in phrases[i-1] and '?' not in phrases[i]:\n",
    "        if y_pred[i]=='React.Respond.':\n",
    "            if speakers[i-1]!=speakers[i] and speakers[i+1]!=speakers[i]:\n",
    "                if len(word_tokenize(phrases[i]))==2:\n",
    "                    doc=nlp(phrases[i])\n",
    "                    for token in doc:\n",
    "                        token_pos=token.pos_\n",
    "                        if token_pos!='VERB':\n",
    "                            y_pred[i]='React.Respond.Support.Register'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sustain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(phrases)):\n",
    "    if speakers[i-1]==speakers[i]:\n",
    "        if phrases[i] not in questions: \n",
    "            if y_pred[i-1]=='Sustain.Continue.':\n",
    "                    if y_pred[i+1]=='Sustain.Continue.':\n",
    "                        y_pred[i]='Sustain.Continue.'\n",
    "                    else:\n",
    "                        if y_pred[i]!='Open.':\n",
    "                            if phrases[i] not in questions:\n",
    "                                y_pred[i]='Sustain.Continue.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(phrases)):\n",
    "    if phrases[i] not in questions:\n",
    "        if phrases[i-1] not in questions:\n",
    "            if speakers[i-1]!=speakers[i]:\n",
    "                if y_pred[i]=='Sustain.Continue.':\n",
    "                    y_pred[i]='React.Respond.Develop.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sustains=[]\n",
    "for i in range(len(phrases)):\n",
    "    if y_pred[i]=='Sustain.Continue.' or y_pred[i]=='React.Respond.Develop.':\n",
    "        test_sustains.append(phrases[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sustains=get_embeddings(sustains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sustains_emb=get_embeddings(test_sustains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "lr_sus = LogisticRegression(C=0.01, class_weight='balanced')\n",
    "lr_sus.fit(train_sustains,sus_tags)\n",
    "tags_for_sus= lr_sus.predict(test_sustains_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_sustains)):\n",
    "    sus_index = phrases.index(test_sustains[i])\n",
    "    if y_pred[sus_index]=='Sustain.Continue.':\n",
    "        y_pred[sus_index]=tags_for_sus[i]\n",
    "    if y_pred[sus_index]=='React.Respond.Develop.':\n",
    "        cut_tags=tags_for_sus[i].split('.')[-1]\n",
    "        if cut_tags!='Monitor':\n",
    "            y_pred[sus_index]+=cut_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(phrases)):\n",
    "    if y_pred[i]=='Sustain.Continue.':\n",
    "        if 'you know' in phrases[i].lower():\n",
    "            y_pred[i]='Sustain.Continue.Monitor'\n",
    "        else:\n",
    "            y_pred[i]='Sustain.Continue.Prolong.Extend'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replies после вопросов\n",
    "try_replies=[]\n",
    "test_prev_lines = []\n",
    "indexes = []\n",
    "test_responds=[]\n",
    "test_responds_prev_lines = []\n",
    "re_indexes = []\n",
    "for i in range(len(speakers)):\n",
    "    if phrases[i-1] in questions:\n",
    "        if y_pred[i]=='React.Respond.':\n",
    "            if speakers[i]!=speakers[i-1]:\n",
    "                test_prev_lines.append(phrases[i-1])\n",
    "                try_replies.append(phrases[i])\n",
    "                indexes.append(i)\n",
    "            else:\n",
    "                test_responds_prev_lines.append(phrases[i-1])\n",
    "                test_responds.append(phrases[i])\n",
    "                re_indexes.append(i)\n",
    "#                 print(speakers[i-1],phrases[i-1],y_pred[i-1]) \n",
    "#                 print(speakers[i],phrases[i],y_pred[i])\n",
    "#                 print(speakers[i+1],phrases[i+1],y_pred[i+1])\n",
    "#                 print('__________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_reply_emb = get_embeddings(try_replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prev = get_embeddings(test_prev_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_concat = np.concatenate([try_reply_emb,test_prev],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embed_replies=get_embeddings(replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prev_lines=get_embeddings(previous_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_concatenate = np.concatenate([train_embed_replies,train_prev_lines], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "lr_reply = LogisticRegression(C=0.01, class_weight='balanced')\n",
    "lr_reply.fit(reply_concatenate,tags)\n",
    "tags_for_reply = lr_reply.predict(test_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(try_replies)):\n",
    "    y_pred[indexes[i]]=y_pred[indexes[i]]+tags_for_reply[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(try_replies)):\n",
    "    if 'yes' in try_replies[i].lower():\n",
    "        if y_pred[indexes[i]]=='React.Respond.Reply.Disagree':\n",
    "            if 'Confirm' in y_pred[indexes[i]-1]:\n",
    "                y_pred[indexes[i]]='React.Respond.Reply.Affirm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(speakers)):\n",
    "    if phrases[i-1] not in questions:\n",
    "        if y_pred[i]=='React.Respond.':\n",
    "            print(speakers[i-1],phrases[i-1],y_pred[i-1]) \n",
    "            print(speakers[i],phrases[i],y_pred[i])\n",
    "            print(speakers[i+1],phrases[i+1],y_pred[i+1])\n",
    "            print('__________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replies после других предложений \n",
    "test_responds=[]\n",
    "test_responds_prev_lines = []\n",
    "re_indexes = []\n",
    "for i in range(len(speakers)):\n",
    "    if phrases[i-1] not in questions:\n",
    "        if y_pred[i]=='React.Respond.':\n",
    "            test_responds_prev_lines.append(phrases[i-1])\n",
    "            test_responds.append(phrases[i])\n",
    "            re_indexes.append(i)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_responds_emb=get_embeddings(test_responds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prev_responds=get_embeddings(test_responds_prev_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_responds_concatenate = np.concatenate([test_responds_emb,test_prev_responds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb_responds=get_embeddings(responds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prev_responds=get_embeddings(previous_responds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "responds_concatenate = np.concatenate([train_emb_responds,train_prev_responds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "lr_responds = LogisticRegression(C=0.5, class_weight='balanced')\n",
    "lr_responds.fit(responds_concatenate,respond_tags)\n",
    "tags_for_responds = lr_responds.predict(test_responds_concatenate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_responds)):\n",
    "    y_pred[re_indexes[i]]=y_pred[re_indexes[i]]+tags_for_responds[i]\n",
    "    if ' no ' in test_responds[i].lower():\n",
    "        y_pred[re_indexes[i]]='Rejoinder.Counter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rejoinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(phrases)):\n",
    "    if y_pred[i]=='React.Rejoinder.':\n",
    "        if 'Develop' in y_pred[i-1]:\n",
    "            if speakers[i-1]==speakers[i+1]:\n",
    "                y_pred[i]='Sustain.Continue.Prolong.Extend'\n",
    "        if ' no ' in phrases[i].lower():\n",
    "            y_pred[i]='React.Rejoinder.Counter'\n",
    "        if phrases[i-1] in questions:\n",
    "            if speakers[i-1]!=speakers[i]:\n",
    "                y_pred[i]='React.Rejoinder.Response.Resolve'\n",
    "            else:\n",
    "                y_pred[i]='React.Rejoinder.Re-challenge'\n",
    "        else:\n",
    "            y_pred[i]='React.Respond.Develop.Extend'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fact or Opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(phrases)):\n",
    "    if y_pred[i]=='Open.':\n",
    "        if len(word_tokenize(phrases[i]))<3:\n",
    "            poses=[]\n",
    "            doc=nlp(phrases[i])\n",
    "            for token in doc:\n",
    "                poses.append(token.pos_)\n",
    "            if 'PROPN' in poses:\n",
    "                y_pred[i]='Open.Attend'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filepath):\n",
    "    documents_f = open(filepath, 'rb')\n",
    "    file = pickle.load(documents_f)\n",
    "    documents_f.close() \n",
    "    return file\n",
    "\n",
    "def divide_into_sentences(document):\n",
    "    return [sent for sent in document.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_fine_grained_pos_tags(sent):\n",
    "    tag_dict = {'-LRB-': 0, '-RRB-': 0, ',': 0, ':': 0, '.': 0, \"''\": 0, '\"\"': 0, '#': 0, \n",
    "    '``': 0, '$': 0, 'ADD': 0, 'AFX': 0, 'BES': 0, 'CC': 0, 'CD': 0, 'DT': 0,\n",
    "    'EX': 0, 'FW': 0, 'GW': 0, 'HVS': 0, 'HYPH': 0, 'IN': 0, 'JJ': 0, 'JJR': 0, \n",
    "    'JJS': 0, 'LS': 0, 'MD': 0, 'NFP': 0, 'NIL': 0, 'NN': 0, 'NNP': 0, 'NNPS': 0, \n",
    "    'NNS': 0, 'PDT': 0, 'POS': 0, 'PRP': 0, 'PRP$': 0, 'RB': 0, 'RBR': 0, 'RBS': 0, \n",
    "    'RP': 0, '_SP': 0, 'SYM': 0, 'TO': 0, 'UH': 0, 'VB': 0, 'VBD': 0, 'VBG': 0, \n",
    "    'VBN': 0, 'VBP': 0, 'VBZ': 0, 'WDT': 0, 'WP': 0, 'WP$': 0, 'WRB': 0, 'XX': 0,\n",
    "    'OOV': 0, 'TRAILING_SPACE': 0}\n",
    "    for token in sent:\n",
    "        if token.is_oov:\n",
    "            tag_dict['OOV']+= 1\n",
    "        elif token.tag_ == '':\n",
    "            tag_dict['TRAILING_SPACE']+= 1\n",
    "        else:\n",
    "            tag_dict[token.tag_]+= 1\n",
    "            \n",
    "    return tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_dependency_tags(sent):\n",
    "    dep_dict = {'acl': 0, 'advcl': 0, 'advmod': 0, 'amod': 0, 'appos': 0, 'aux': 0, 'case': 0,\n",
    "    'cc': 0, 'ccomp': 0, 'clf': 0, 'compound': 0, 'conj': 0, 'cop': 0, 'csubj': 0,\n",
    "    'dep': 0, 'det': 0, 'discourse': 0, 'dislocated': 0, 'expl': 0, 'fixed': 0,\n",
    "    'flat': 0, 'goeswith': 0, 'iobj': 0, 'list': 0, 'mark': 0, 'nmod': 0, 'nsubj': 0,\n",
    "    'nummod': 0, 'obj': 0, 'obl': 0, 'orphan': 0, 'parataxis': 0, 'prep': 0, 'punct': 0,\n",
    "    'pobj': 0, 'dobj': 0, 'attr': 0, 'relcl': 0, 'quantmod': 0, 'nsubjpass': 0,\n",
    "    'reparandum': 0, 'ROOT': 0, 'vocative': 0, 'xcomp': 0, 'auxpass': 0, 'agent': 0,\n",
    "    'poss': 0, 'pcomp': 0, 'npadvmod': 0, 'predet': 0, 'neg': 0, 'prt': 0, 'dative': 0,\n",
    "    'oprd': 0, 'preconj': 0, 'acomp': 0, 'csubjpass': 0, 'meta': 0, 'intj': 0, \n",
    "    'TRAILING_DEP': 0}\n",
    "    \n",
    "    for token in sent:\n",
    "        if token.dep_== '':\n",
    "            dep_dict['TRAILING_DEP']+= 1\n",
    "        else:\n",
    "            try:\n",
    "                dep_dict[token.dep_]+= 1\n",
    "            except:\n",
    "                print('Unknown dependency for token: \"' + token.orth_ +'\". Passing.')\n",
    "        \n",
    "    return dep_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_specific_entities(sent):\n",
    "    entity_dict = {\n",
    "    'PERSON': 0, 'NORP': 0, 'FAC': 0, 'ORG': 0, 'GPE': 0, 'LOC': 0,\n",
    "    'PRODUCT': 0, 'EVENT': 0, 'WORK_OF_ART': 0, 'LAW': 0, 'LANGUAGE': 0,\n",
    "    'DATE': 0, 'TIME': 0, 'PERCENT': 0, 'MONEY': 0, 'QUANTITY': 0,\n",
    "    'ORDINAL': 0, 'CARDINAL': 0 }    \n",
    "    entities = [ent.label_ for ent in sent.as_doc().ents]\n",
    "    for entity in entities:\n",
    "        entity_dict[entity]+=1     \n",
    "    return entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_sent, classifier, scaler=None):\n",
    "    parsed_test = divide_into_sentences(nlp(test_sent))\n",
    "    # Get features\n",
    "    sentence_with_features = {}\n",
    "    entities_dict = number_of_specific_entities(parsed_test[0])\n",
    "    sentence_with_features.update(entities_dict)\n",
    "    pos_dict = number_of_fine_grained_pos_tags(parsed_test[0])\n",
    "    sentence_with_features.update(pos_dict)\n",
    "    #dep_dict = number_of_dependency_tags(parsed_test[0])\n",
    "    #sentence_with_features.update(dep_dict)\n",
    "    df = pd.DataFrame(sentence_with_features, index=[0])\n",
    "    if scaler:\n",
    "        df = scaler.transform(df)\n",
    "    \n",
    "    prediction = classifier.predict(df)\n",
    "    if prediction == 0:\n",
    "        open_list.append('Fact')\n",
    "    else:\n",
    "        open_list.append('Opinion')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_classifier = load_pickle('nn_classifier.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = load_pickle('scaler.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_phrases=[]\n",
    "open_index=[]\n",
    "for i in range(len(phrases)):\n",
    "    if y_pred[i]=='Open.':\n",
    "        predict(phrases[i],nn_classifier,scaler)\n",
    "        open_phrases.append(phrases[i])\n",
    "        open_index.append(i)\n",
    "#         print(speakers[i-1],phrases[i-1],y_pred[i-1]) \n",
    "#         print(speakers[i],phrases[i],y_pred[i])\n",
    "#         if i+1 < len(phrases):\n",
    "#             print(speakers[i+1],phrases[i+1],y_pred[i+1])\n",
    "#         print('__________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(open_phrases)):\n",
    "    if open_list[i]=='Fact':\n",
    "        if open_phrases[i] not in questions:\n",
    "            open_list[i]='Give.Fact.'\n",
    "        else:\n",
    "            open_list[i]='Demand.Fact.'\n",
    "    else:\n",
    "        if open_phrases[i] not in questions:\n",
    "            open_list[i]='Give.Opinion.'\n",
    "        else:\n",
    "            open_list[i]='Demand.Opinion.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(open_phrases)):\n",
    "    y_pred[open_index[i]]=y_pred[open_index[i]]+open_list[i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
